[package]
name = "nanochat-inference"
version.workspace = true
edition.workspace = true
rust-version.workspace = true
license.workspace = true
authors.workspace = true
description = "OpenAI-compatible inference server for nanochat"
repository = "https://github.com/karpathy/nanochat"

[[bin]]
name = "nanochat-inference"
path = "src/main.rs"

[dependencies]
nanochat-model = { path = "../nanochat-model" }
nanochat-tokenizer = { path = "../nanochat-tokenizer" }
aprender = { workspace = true, features = ["gpu"] }
actix-web = { workspace = true, features = ["macros"] }
tokio = { workspace = true, features = ["rt-multi-thread", "macros", "signal"] }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
pretty_assertions = { workspace = true }

[features]
default = ["gpu"]
gpu = ["aprender/gpu"]
cpu-only = []

